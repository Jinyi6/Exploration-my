# 值分布 Critic (Distributional Critic) 方案记录

本文档详细记录了当前代码库中 Distributional Critic 的实现原理、方案细节以及其在 PPO 算法中的具体作用。

## 1. 核心理念

传统的 Critic 输出状态 $s$ 的期望回报 $V(s) = \mathbb{E}[R \mid s]$，是一个标量。
**Distributional Critic** 则试图学习回报的完整概率分布 $Z(s)$，即 $Z(s) \stackrel{D}{=} R \mid s$。

引入分布信息的目的通常包括：
1.  **更丰富的训练信号**：分布包含比均值更多的信息（如多峰、方差等），能辅助 Critic 学习更好的特征表示。
2.  **训练稳定性**：在某些场景下，分布式 RL 比标量 RL 收敛更稳定。
3.  **不确定性量化**：虽然目前主要只用了期望，但分布信息本身蕴含了对未来的不确定性预估。

## 2. 实现方案概览

代码通过 `config.distributional` 开关控制，支持三种具体的分布建模模式 (`config.quantile_mode`)：

| 模式 | 全称 | 原理简述 | 输出形式 | 损失函数 |
| :--- | :--- | :--- | :--- | :--- |
| **iqn** (默认) | Implicit Quantile Networks | 将分位点 $\tau \sim U([0,1])$ 作为输入，动态输出对应的分位值。 | 输入 Hidden + $\tau$，输出 Quantile Value | Quantile Huber Loss |
| **fixed** | Fixed Quantile Regression | 类似于 QR-DQN，输出固定分位点（如 0.1, 0.2 ...）对应的分位值。 | 输入 Hidden，输出 $K$ 个 Quantile Values | Quantile Huber Loss |
| **c51** | Categorical 51 | 将回报值域离散化为固定的原子 (Atoms)，预测概率分布。 | 输入 Hidden，输出 $N$ 个 Atoms 的 Logits | Projected Cross Entropy |

### 2.1 架构设计

在 `verl/verl/workers/critic/dp_critic.py` 和 `utils/model.py` 中，根据模式挂载不同的 Head：

*   **Fixed / IQN**:
    *   **Fixed**: 使用 `QRValueHead`，直接输出 $K$ 个标量（分位点的值）。
    *   **IQN**: 使用 `IQNValueHead`，除了 Hidden States 外，还额外接收采样到的 $\tau$ (Taus) 嵌入，输出对应的分位值。
*   **C51**:
    *   使用 `C51ValueHead`，输出 $N$ 个 Logits（对应 $N$ 个固定支撑点 Atoms）。支撑点由 `c51_v_min` 和 `c51_v_max` 决定。

### 2.2 前向传播 (Forward) 与 期望推断

尽管 Critic 内部维护了分布，但 PPO 的 Advantage 计算目前依赖标量 Baseline。因此，在前向推理阶段，我们需要**将分布坍缩为期望**。

*   **IQN / Fixed**:
    *   预测出 $K$ 个分位点的值 $q_i$。
    *   $V(s) \approx \frac{1}{K} \sum_{i=1}^K q_i$ (直接取均值作为期望估计)。
    *   *注：IQN 在推理时也会随机采样 $\tau$，因此其输出的 Baseline 带有一定的随机性（Monte Carlo 估计）。*
*   **C51**:
    *   预测出 Logits，经 Softmax 得到概率 $p_i$。
    *   $V(s) = \sum_{i=1}^N p_i \cdot z_i$，其中 $z_i$ 是固定的 Atom 值。

这一步发生在 `dp_critic.py` 的 `compute_values` 函数中。

### 2.3 训练目标 (Loss)

Critic 的训练目标是最小化预测分布与真实回报 Target 之间的距离。
这里的 Target 是 PPO 采样得到的 **Execute Return** (Monte Carlo Returns) 或者 GAE 估算的 Returns。这是一个标量，但在计算 Loss 时会被视为一个退化的分布（Dirac delta function）。

1.  **IQN / Fixed (Quantile Regression)**
    *   使用 **Quantile Regression Huber Loss**。
    *   目标是让预测的分位点 $q_\tau$ 逼近 Target Return。
    *   公式核心：$\rho_\tau(u) = u \cdot (\tau - \mathbb{I}(u < 0))$，其中 $u = y - q_\tau$。

2.  **C51 (Categorical Projection)**
    *   使用 **Projected Categorical Cross Entropy Loss**。
    *   **投影 (Projection)**: 将标量 Target Return 投影到固定的 Atoms 支撑集上。由于 Return 可能落在两个 Atom 之间，通过线性插值将均值分配给最近的两个 Atom（软 Label）。
    *   **交叉熵**: 计算预测概率 $\log p$ 与投影后的 Target 分布之间的 KL 散度（即 Cross Entropy）。

## 3. 对 PPO 算法公式的影响

在当前的实现中，Distributional Critic **仅改变了 Critic 的训练方式和 Baseline 的估算准确度**，并未修改 PPO Actor 的更新公式。

### 3.1 Actor 更新
标准 PPO Policy Gradient:
$$ \nabla_\theta J(\theta) \approx \mathbb{E} \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] $$
其中 Advantage $A_t$ 的计算依赖于 Critic 提供的 $V(s_t)$：
$$ A_t^{GAE} = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$

**Distributional 的影响**：
*   $V(s_t)$ 不再是网络直接输出的标量，而是分布 $Z(s_t)$ 的期望 $\mathbb{E}[Z(s_t)]$。
*   如果 Critic 的分布建模让 $V(s_t)$ 的估计更准确或泛化性更强，那么 $A_t$ 的估计就会更准确，从而间接提升 Policy 的学习效果。

### 3.2 基础版本：不涉及 Risk-Sensitive

最初的实现中，Distributional Critic **只用于提供更强的标量 Baseline**，并不显式使用分布的高阶信息（如 VaR, CVaR）来修改优势函数或优化目标：

- Critic 学习 $Z(s)$，但在 Actor 侧只取 $\mathbb{E}[Z(s)]$ 作为 $V(s)$。
- Advantage / Returns 的定义与标准 PPO 一致，目标仍然是最大化 $\mathbb{E}[G]$。

在此基础上，我们后来引入了 **风险泛函 $\rho(Z)$** 以及一系列 `risk_*` 参数，使得：

- 可以用 $\rho(Z)$ 替换 Baseline，起到更稳健/更保守的方差控制作用；
- 也可以直接用风险目标（如 CVaR）来定义优化目标，实现真正的 Risk-Sensitive PPO。

下面一节记录的是这部分 Risk-Sensitive 扩展的最新设计和实际实现状态。

## 4. 总结

| 组件 | 标准 PPO Critic | Distributional Critic (当前实现) |
| :--- | :--- | :--- |
| **输出** | 标量 $V$ | 分布 $Z$ (Quantiles 或 Atoms Prob) |
| **推理值** | 直接输出 | $\mathbb{E}[Z]$ (均值) |
| **Loss** | MSE Loss $(V - R)^2$ | Quantile Huber Loss 或 C51 Cross Entropy |
| **作用** | 提供 Baseline | 提供更强 Baseline，更稳定的 Critic 训练 |

此方案的“基础版”只利用分布强化学习 (Distributional RL) 的辅助任务特性来增强 Critic 的表征能力，而后续的 Risk-Sensitive 扩展则在此之上，显式改变 PPO 的优化目标或 Baseline。

# Risk-Sensitive PPO 扩展（最新设计）

在 Distributional Critic 的基础上，我们通过风险泛函 $\rho(Z)$ 引入了风险偏好（Risk Sensitivity），允许算法利用分布的尾部行为（如 CVaR）来：

- 调整优化目标 $G$（例如下尾 CVaR 代表风险厌恶目标）；
- 或仅调整 Baseline $b(s)$（控制方差，不改变目标）。

关键参数包括两层：

- Critic 侧：`critic.risk_apply_to` + `critic.risk_level`  
- 算法侧：`algorithm.risk_apply_to` + `algorithm.baseline_mode` + `algorithm.baseline_mix_beta`

## 1. 风险泛函与 Critic 输出

### 1.1 `risk_level`：定义 $\rho(Z)$ 的形状

配置路径：`critic.risk_level` 和 `algorithm.risk_level` 共用同一枚举。

- `"neutral"`：中性，无风险偏好，对应 $\rho(Z) = \mathbb{E}[Z]$。
- `"averse"` / `"cvar_lower_0.1"`：风险厌恶，下尾 CVaR；
- `"seeking"` / `"cvar_upper_0.1"`：风险寻求，上尾 CVaR；
- 支持更一般的格式：`cvar_lower_α` 或 `cvar_upper_α`。

### 1.2 Critic 侧 `critic.risk_apply_to`

配置路径：`critic.risk_apply_to`，影响 `dp_critic.py::compute_values` 的行为。

- `"none"`：Distributional Critic 只输出 $\mathbb{E}[Z]$ 作为 `values`；
- `"baseline"` / `"target"` / `"target1"` / `"baseline1"` 且 `risk_level != "neutral"`：  
  Distributional Critic 会通过 `compute_rho_from_dist` 将分布 $Z(s)$ 映射为风险值：
  - IQN / Fixed：在 quantiles 上按 `risk_level` 聚合；
  - C51：在离散 atoms 上按 `risk_level` 计算 CVaR。

因此，**Critic 的 `values` 是否是 “风险值” 由 `critic.risk_apply_to` + `critic.risk_level` 控制**，供后续 Advantage 计算使用。

## 2. 算法侧 risk_apply_to：G 与 b(s) 的语义

配置路径：`algorithm.risk_apply_to`，影响 `ray_trainer.py::compute_advantage`。

目前支持的取值及语义：

- `"none"`（默认）  
  - 不启用额外的风险逻辑；
  - 按标准 PPO / GAE 路径计算 `returns` 与 `advantages`。

- `"baseline"`（旧方案，保留兼容，不建议新实验使用）  
  - 利用 Distributional Critic 的风险值作为 GAE 的 Baseline；
  - 目标 $G$ 仍然是原始 returns，但 TD 递推中的 $V(s)$ 已是 $\rho(Z)$；
  - 容易将“风险值”卷入 TD 递推结构，语义略重，推荐用下面的 `baseline1` 替代。

- `"target"`（旧 Risk-of-Risk Target，**已知存在自闭环问题，视为 deprecated**）  
  - 行为（保留供兼容）：  
    - 直接取 Critic 输出 `values` 的末 token，记为 $\hat{\rho}$；  
    - 将 $\hat{\rho}$ 广播为整条序列的 `returns`，并令 `advantages = returns`；  
    - Critic 使用这个被覆盖的 `returns` 作为 supervision，形成 “critic 用自己的输出监督自己” 的闭环。  
  - 建议只作为历史实验兼容，不再用于新实验。

- `"target1"`（新版 Risk-Sensitive 目标，修复自闭环）  
  - 计算过程拆成两步：
    1. 先按标准 GAE 基于 token-level rewards 和 Critic 的 `values` 得到基础 `returns`（真实回报信号），用于 Critic 训练；
    2. 再用 Critic 的 `values` 末 token 得到 G 的风险版本：
       - 若 Critic 配置为 risk 模式：`G = ρ(Z_last)`（如下尾 CVaR）；  
       - 若 Critic 为 neutral：`G = E[Z_last]`。
  - 只用**风险目标 G** 来重写 actor 的 `advantages`，并且：
    - **不再覆盖 `data.batch["returns"]`**，Critic 仍然对真实 returns 进行分布学习；
    - 是否减 baseline 由 `baseline_mode` 控制（见下节）。

- `"baseline1"`（新版 Baseline-only 模式，仅改 baseline，不改 GAE 递推结构）  
  - 先用标准 GAE 得到 `returns`（真实目标）；
  - 把 G 固定为 `returns`，只对 baseline 做 control variate：
    - `A_{i,t} = G_{i,t} - b(s_{i,t})`；
    - 其中 `b(s)` 来自 Critic 的 `values`（是否为 risk 由 Critic 配置决定）；
    - 只修改 actor 的 `advantages`，`returns` 本身不变，Critic 仍然学习真实回报。

可以总结为：

- **`target1`：G 会按 risk_level 变成“风险目标”（如 CVaR），从而真正“优化风险目标”；**
- **`baseline1`：G 不变（仍是原 PPO 的 returns），只改 b(s) 降低方差，不改变目标本身。**

## 3. baseline_mode / baseline_mix_beta

配置路径：`algorithm.baseline_mode` / `algorithm.baseline_mix_beta`，同样在 `compute_advantage` 中生效。

### 3.1 当前代码中的行为（实际实现）

在当前实现中：

- `baseline_mode == "none"`：  
  - 完全不减 baseline，`b(s) = 0`，优势估计方差较大；
- `baseline_mode != "none"`（`"neutral"` / `"risk"` / `"mix"` 等）：
  - 统一视为“开启 baseline”，使用 Critic 的 `values` 作为 baseline：  
    `b(s) = values(s)`；
  - `values` 是中性还是风险值，由 **Critic 侧** 的 `critic.risk_apply_to` + `critic.risk_level` 决定。

也就是说，**目前 baseline_mode 的字符串值主要区分“开 / 关 baseline”**，而：

- “baseline 是 V(s) 还是 ρ(Z(s))” 由 Critic 配置决定；
- `baseline_mix_beta` 目前仅作为预留参数，未来用于显式实现：  
  `b(s) = β V(s) + (1-β) ρ(Z(s))`。

### 3.2 设计语义（面向未来扩展）

从设计角度，我们希望：

- `"neutral"`：优先使用 $\mathbb{E}[Z]$ 做 Baseline；
- `"risk"`：使用 $\rho(Z)$ 做 Baseline；
- `"mix"`：按 `baseline_mix_beta` 在两者之间线性插值；

为此需要 Distributional Critic 同时导出两路值：`V_neutral` 和 `ρ_risk`，然后在 `compute_advantage` 中根据 baseline_mode 和 baseline_mix_beta 组合出最终的 $b(s)$。这部分接口已经在配置层预留，后续可以平滑演进实现。

## 4. 典型配置示例（与当前实现兼容）

以下示例都假设我们使用 GAE 作为优势估计器（`adv_estimator: gae`）。

### 4.1 只降方差（baseline1，推荐起步）

```yaml
critic:
  distributional: true
  risk_apply_to: "baseline1"   # 或 "none" + neutral 期望
  risk_level: "neutral"        # values = E[Z(s)]

algorithm:
  adv_estimator: gae
  risk_apply_to: "baseline1"
  baseline_mode: "risk"        # 当前实现中：只表示“启用 baseline”
```

语义：

- G：仍是标准 GAE 的 returns（真实回报）；
- b(s)：用 Critic 的 `values` 作为 Baseline（此时 `values ≈ E[Z]`）；
- 效果：目标不变，只通过更好的 Baseline 降低方差、提升稳定性。

### 4.2 风险厌恶目标（target1），修复闭环 + 减 baseline

```yaml
critic:
  distributional: true
  risk_apply_to: "target1"
  risk_level: "cvar_lower_0.1"   # 下尾 CVaR

algorithm:
  adv_estimator: gae
  risk_apply_to: "target1"
  baseline_mode: "neutral"       # 当前实现：只要 != "none" 就会用 values 做 baseline
```

语义：

- Critic 的 `values` ≈ $\text{CVaR}_{\text{lower-}0.1}[Z(s)]$；
- G：用 `values` 的末 token 作为风险目标 $G = \rho(Z_{last})$；
- b(s)：同样使用 `values`（即风险值）作为 Baseline；
- Critic 使用 GAE 得到的真实 returns 训练，不再被风险目标覆盖。

### 4.3 风险目标 + 只改 baseline（baseline1）

如果只想用风险值做 Baseline，而目标 G 仍保持标准 GAE returns：

```yaml
critic:
  distributional: true
  risk_apply_to: "baseline1"
  risk_level: "cvar_lower_0.1"   # 或 "cvar_upper_0.1"

algorithm:
  adv_estimator: gae
  risk_apply_to: "baseline1"
  baseline_mode: "risk"
```

语义：

- Critic 的 `values` 是风险值（例如下尾 CVaR）；
- G：标准 GAE returns（真实回报）；
- b(s)：风险 Baseline `b = ρ(Z(s))`；
- 效果：保持原 PPO 目标，只通过风险 Baseline 改善方差与稳定性。

> 注：上述示例与当前代码实现保持一致；后续若完善 baseline_mode 的 neutral / risk / mix 细分语义，本节可以在不改整体结构的前提下，进一步细化每种组合的行为描述。 
