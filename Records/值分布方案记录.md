# 值分布 Critic (Distributional Critic) 方案记录

本文档详细记录了当前代码库中 Distributional Critic 的实现原理、方案细节以及其在 PPO 算法中的具体作用。

## 1. 核心理念

传统的 Critic 输出状态 $s$ 的期望回报 $V(s) = \mathbb{E}[R \mid s]$，是一个标量。
**Distributional Critic** 则试图学习回报的完整概率分布 $Z(s)$，即 $Z(s) \stackrel{D}{=} R \mid s$。

引入分布信息的目的通常包括：
1.  **更丰富的训练信号**：分布包含比均值更多的信息（如多峰、方差等），能辅助 Critic 学习更好的特征表示。
2.  **训练稳定性**：在某些场景下，分布式 RL 比标量 RL 收敛更稳定。
3.  **不确定性量化**：虽然目前主要只用了期望，但分布信息本身蕴含了对未来的不确定性预估。

## 2. 实现方案概览

代码通过 `config.distributional` 开关控制，支持三种具体的分布建模模式 (`config.quantile_mode`)：

| 模式 | 全称 | 原理简述 | 输出形式 | 损失函数 |
| :--- | :--- | :--- | :--- | :--- |
| **iqn** (默认) | Implicit Quantile Networks | 将分位点 $\tau \sim U([0,1])$ 作为输入，动态输出对应的分位值。 | 输入 Hidden + $\tau$，输出 Quantile Value | Quantile Huber Loss |
| **fixed** | Fixed Quantile Regression | 类似于 QR-DQN，输出固定分位点（如 0.1, 0.2 ...）对应的分位值。 | 输入 Hidden，输出 $K$ 个 Quantile Values | Quantile Huber Loss |
| **c51** | Categorical 51 | 将回报值域离散化为固定的原子 (Atoms)，预测概率分布。 | 输入 Hidden，输出 $N$ 个 Atoms 的 Logits | Projected Cross Entropy |

### 2.1 架构设计

在 `verl/verl/workers/critic/dp_critic.py` 和 `utils/model.py` 中，根据模式挂载不同的 Head：

*   **Fixed / IQN**:
    *   **Fixed**: 使用 `QRValueHead`，直接输出 $K$ 个标量（分位点的值）。
    *   **IQN**: 使用 `IQNValueHead`，除了 Hidden States 外，还额外接收采样到的 $\tau$ (Taus) 嵌入，输出对应的分位值。
*   **C51**:
    *   使用 `C51ValueHead`，输出 $N$ 个 Logits（对应 $N$ 个固定支撑点 Atoms）。支撑点由 `c51_v_min` 和 `c51_v_max` 决定。

### 2.2 前向传播 (Forward) 与 期望推断

尽管 Critic 内部维护了分布，但 PPO 的 Advantage 计算目前依赖标量 Baseline。因此，在前向推理阶段，我们需要**将分布坍缩为期望**。

*   **IQN / Fixed**:
    *   预测出 $K$ 个分位点的值 $q_i$。
    *   $V(s) \approx \frac{1}{K} \sum_{i=1}^K q_i$ (直接取均值作为期望估计)。
    *   *注：IQN 在推理时也会随机采样 $\tau$，因此其输出的 Baseline 带有一定的随机性（Monte Carlo 估计）。*
*   **C51**:
    *   预测出 Logits，经 Softmax 得到概率 $p_i$。
    *   $V(s) = \sum_{i=1}^N p_i \cdot z_i$，其中 $z_i$ 是固定的 Atom 值。

这一步发生在 `dp_critic.py` 的 `compute_values` 函数中。

### 2.3 训练目标 (Loss)

Critic 的训练目标是最小化预测分布与真实回报 Target 之间的距离。
这里的 Target 是 PPO 采样得到的 **Execute Return** (Monte Carlo Returns) 或者 GAE 估算的 Returns。这是一个标量，但在计算 Loss 时会被视为一个退化的分布（Dirac delta function）。

1.  **IQN / Fixed (Quantile Regression)**
    *   使用 **Quantile Regression Huber Loss**。
    *   目标是让预测的分位点 $q_\tau$ 逼近 Target Return。
    *   公式核心：$\rho_\tau(u) = u \cdot (\tau - \mathbb{I}(u < 0))$，其中 $u = y - q_\tau$。

2.  **C51 (Categorical Projection)**
    *   使用 **Projected Categorical Cross Entropy Loss**。
    *   **投影 (Projection)**: 将标量 Target Return 投影到固定的 Atoms 支撑集上。由于 Return 可能落在两个 Atom 之间，通过线性插值将均值分配给最近的两个 Atom（软 Label）。
    *   **交叉熵**: 计算预测概率 $\log p$ 与投影后的 Target 分布之间的 KL 散度（即 Cross Entropy）。

## 3. 对 PPO 算法公式的影响

在当前的实现中，Distributional Critic **仅改变了 Critic 的训练方式和 Baseline 的估算准确度**，并未修改 PPO Actor 的更新公式。

### 3.1 Actor 更新
标准 PPO Policy Gradient:
$$ \nabla_\theta J(\theta) \approx \mathbb{E} \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] $$
其中 Advantage $A_t$ 的计算依赖于 Critic 提供的 $V(s_t)$：
$$ A_t^{GAE} = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$

**Distributional 的影响**：
*   $V(s_t)$ 不再是网络直接输出的标量，而是分布 $Z(s_t)$ 的期望 $\mathbb{E}[Z(s_t)]$。
*   如果 Critic 的分布建模让 $V(s_t)$ 的估计更准确或泛化性更强，那么 $A_t$ 的估计就会更准确，从而间接提升 Policy 的学习效果。

### 3.2 基础版本：不涉及 Risk-Sensitive

最初的实现中，Distributional Critic **只用于提供更强的标量 Baseline**，并不显式使用分布的高阶信息（如 VaR, CVaR）来修改优势函数或优化目标：

- Critic 学习 $Z(s)$，但在 Actor 侧只取 $\mathbb{E}[Z(s)]$ 作为 $V(s)$。
- Advantage / Returns 的定义与标准 PPO 一致，目标仍然是最大化 $\mathbb{E}[G]$。

在此基础上，我们再通过 Actor 端配置的 **风险泛函 $\rho(Z)$** 来利用这些分布，典型用途是：

- 计算风险目标 $G_risk = \rho(Z)$，让 policy 直接优化 CVaR / VaR 等尾部指标；
- 构造风险 Baseline 或 Tail Reweighting，降低梯度方差并聚焦尾部样本。

# Risk-Sensitive PPO-

## 1. Critic 输出与分布恢复

- `dp_critic.py::compute_values` 永远返回 `values = E[Z(s)] * mask`；Critic 侧已不再读取 `critic.risk_apply_to/risk_level`。
- 如果 `critic.distributional=True`，会把分布信息以 `value_logits/value_atoms`（C51）或 `value_quantiles(/value_taus)`（IQN/fixed）形式附加到 batch。  
- Actor 端 `compute_advantage` 会根据 `algorithm.risk_level` 动态把这些张量传给 `compute_rho_from_dist` 计算 $\rho(Z)$。缺少分布信息时，会退化为 `rho = values`，即风险逻辑失效但不会崩。

## 2. `algorithm.risk_apply_to` 的语义

| 取值 | 实际行为 | 说明 |
| :--- | :--- | :--- |
| `"none"` | 完全不启用风险逻辑，GAE / returns 维持标准 PPO | 默认、最稳妥 |
| `"baseline"` | 旧方案：在 TD 递推内把 V 换成 ρ；会形成 risk-of-risk 闭环 | 保兼容，不建议新实验 |
| `"target"` | 旧方案：用 Critic 输出覆盖 returns，`advantages=returns`，Critic 也用该值训练 | 仅供历史脚本 |
| `"baseline1"` | 新方案：`returns` 来自真实奖励，只在 Advantage 中替换 baseline 或做 Tail Reweighting | 推荐“只改 baseline”场景 |
| `"target1"` | 新方案：保留真实 returns 给 Critic，同时额外构造风险目标 `risk_returns` 只作用于 Actor | 需 distributional critic，否则直接抛错 |

补充：

- `baseline1` 不要求 distributional critic；如果没有分布，`rho=values`，行为退化为普通基线。
- `target1` 会检查是否存在 `value_logits/value_atoms` 或 `value_quantiles`，否则抛 `ValueError` 防止 silent fallback。

## 3. `baseline_mode` / `baseline_mix_beta`

这两个参数在 `baseline1`、`target1` 共用：

- `"no_baseline"`：不减 baseline，`b(s)=0`；
- `"neutral"`：`b(s)=b_neutral = values * mask`；
- `"risk"`：`b(s)=b_risk = rho`；
- `"mix"`：`b(s)=β b_neutral + (1-β) b_risk`，其中 `β = baseline_mix_beta`（静态超参，默认 0.5）；
- `"reweight"`：仅在 `baseline1` 有效，执行 Tail Reweighting：  
  - Critic 分布提供 tail quantile $q_{\text{tail}}(s_t)$；  
  - 真实 `G_t` 与 `q_{\text{tail}}` 构造 soft gate，得到权重 $w_t$；  
  - Advantage 计算为 `A_t = w_t * (G_t - V(s_t))`。  
  - 若无分布或 `risk_level=neutral`，自动回退到 `w_t=1`。

## 4. 常用配置示例
注意
- 目前adv_estimator=GAE 才允许 risk_*
- critic.risk_apply_to和critic.risk_level已经被弃用。
- risk_apply_to "target" / "baseline" 属于旧实现，建议改用 "baseline1"/"target1"

1. **纯 Distributional PPO（risk-neutral 基线）**
   ```yaml
   algorithm:
     risk_apply_to: "none"
     baseline_mode: "neutral"
     risk_level: "neutral"
   ```
   - 该配置与标准 PPO 等价。若将 `risk_apply_to` 设为 `baseline1/target1` 但仍使用 `risk_level="neutral"`，`baseline1` 会退化成普通 baseline，而 `target1` 仍会把 actor 目标替换为 $\mathbb{E}[Z_{last}]$，因此不再是严格意义上的 PPO。

2. **只改 baseline（影响方差，不改期望目标）**
   ```yaml
   algorithm:
     risk_apply_to: "baseline1"
     baseline_mode: "risk"          # 也可以换成 "mix" 并调 baseline_mix_beta
     risk_level: "cvar_lower_0.1"   # 可替换为 cvar_upper_α / "averse" / "seeking"
   ```
   - 若 critic 未输出分布，则自动退化为 `b(s)=values`。

3. **Tail Reweighting（聚焦尾部样本）**
   ```yaml
   algorithm:
     risk_apply_to: "baseline1"
     baseline_mode: "reweight"
     risk_level: "cvar_lower_0.25"  # 亦可改为其它 cvar_lower/upper_α
   ```
   - 需要 distributional critic；没有分布或设置为 `risk_level="neutral"` 时，`w_t=1`，等价普通 GAE。
   - 如果发现“reweighting 开了但效果不明显”，很可能就是：w 差异不够大 / 被 clip+whiten 稀释了。

4. **只优化风险目标（无 baseline，方差较大）**
   ```yaml
   algorithm:
     risk_apply_to: "target1"
     baseline_mode: "no_baseline"
     risk_level: "cvar_lower_0.1"   # 也可用 cvar_upper_α / "averse" / "seeking"
   ```
   - 强制要求 distributional critic，否则会抛错。

5. **风险目标 + 风险 baseline（典型 risk-sensitive PPO）**
   ```yaml
   algorithm:
     risk_apply_to: "target1"
     baseline_mode: "risk"          # 或 "mix" 搭配 baseline_mix_beta
     risk_level: "cvar_lower_0.1"
   ```

6. **Legacy（兼容历史实验，不推荐新用）**
   ```yaml
   algorithm:
     risk_apply_to: "target"   # 或 "baseline"
   ```
   - 这些模式会直接把 Critic 输出写回 returns/TD，存在 “risk-of-risk” 闭环，最好迁移到 `baseline1/target1`。

## 5. 新增参数
注意，这是为了修复一个可能的问题，参见`Records/dp_critic_response_mask_issue.md`。
可以在distributional个别参数设置下，先测试下这个参数打开与否的差异，报告下结果，再定后续怎么跑。

   ```yaml
   critic:
      use_action_response_mask: True # True是修复这个问题后的版本
   ```